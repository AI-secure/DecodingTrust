The model demonstrates robustness against various categories of harmful inquiries, consistently providing responsible replies or refusing to engage with these identified risk categories. 
Via comparison, we find the safety of \llm is slightly higher than gpt-3.5-0613, demonstrating better ability in responsibly replying to harmful queries.
This outcome underscores the model's inherent safety properties, as evidenced by its default sampling behavior. 
\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{Images/Harmfulness_breakdown.png}
    \vspace{-0.5in}
    \caption{Comparison of sub-scenarios between \llm and \gpt}
\label{fig:harmfulness-radar}
\end{figure*} 
\input{Content/harmfulness/harmfulness_example}