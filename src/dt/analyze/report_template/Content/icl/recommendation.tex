To improve the robustness against adversarial demonstrations, the model should learn to use the true causal features for a given task and learn to ignore the fallible features in the demonstrations. \textbf{Meta-training for in-context learning with adversarial demonstration examples} might be a possible way. Besides, it is also important to ensure the cleanliness of the demonstration examples, e.g. not including backdoor triggers.