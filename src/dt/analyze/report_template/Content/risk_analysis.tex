\section{Risk Analysis}
To further analyze each potential trustworthiness risk, we begin by offering a concise introduction and outlining the goals for each evaluation scenario. Subsequently, we provide a detailed analysis and present failure examples of the \llm on its potential risks. For experimentation details, please refer to our paper \cite{wang2023decodingtrust}. 
\subsection{Overview}
\subsubsection*{Toxicity}
        \begin{itemize}
        \item \textbf{Standard benchmark:} We evaluate on standard benchmark \textsc{RealToxicityPrompts} \cite{gehman2020realtoxicityprompts} to measure the properties and limitations of GPT-3.5 and GPT-4 compared to existing LLM counterparts.
        \item \textbf{Diverse system prompts:} We evaluate the models against our manually designed 33 diverse system prompts (e.g., role-playing, saying the opposite, and replacing word meaning, etc.), designed to evaluate the impact of system prompts on the toxicity level of responses generated by GPT models.
        \item \textbf{Challenging user prompts:} We evaluate the model on our 1.2K challenging user prompts generated by GPT-4 and GPT-3.5, designed to more effectively uncover model toxicity than the existing benchmarks.

    \end{itemize}
\subsubsection*{Adversarial Robustness}
        \begin{itemize}
        \item \textbf{Adversarial SST-2:} We evaluate on adversarial sentiment classification dataset \cite{socher-etal-2013-recursive} generated by attacking autoregressive language models such as Alpaca.
        \item \textbf{Adversarial QQP:} We evaluate on adversarial duplicate question detection dataset generated by attacking autoregressive language models such as Alpaca.
        \item \textbf{Adversarial MNLI:} We evaluate on adversarial natural language inference dataset \cite{MNLI} generated by attacking autoregressive language models such as Alpaca.
    \end{itemize}
\subsubsection*{Out-of-Distribution Robustness}
    \begin{itemize}
        \item \textbf{OOD Style:} We evaluate on datasets with uncommon text styles (e.g., Bible style) that may fall outside the training or instruction tuning distribution, with the goal of assessing the robustness of the model when the input style is uncommon.
        \item \textbf{OOD Knowledge:} We evaluate questions that can only be answered with knowledge after the training data was collected, aiming to investigate the trustworthiness of the model’s responses when the questions are out of scope. (e.g., whether the model knows to refuse to answer unknown questions.
        \item \textbf{OOD In-context Demonstrations:} We evaluate how in-context demonstrations that are purposely drawn from different distributions or domains from the test inputs can affect the final performance of models. 
    \end{itemize}
\subsubsection*{Robustness to Adversarial Demonstrations}
        \begin{itemize}
        \item \textbf{Counterfactual Demonstrations:} We evaluate in-context learning results when adding a counterfactual example of the testing input (a superficially-similar example with a different label) in the demonstrations, with goal of the evaluating if such manipulated demonstrations can mislead the model.
        \item \textbf{Spurious Correlations in Demonstrations:} We evaluate in-context learning results when adding spurious correlations in the demonstrations, with the goal of evaluating if such manipulated demonstrations can mislead the model.
        \item \textbf{Backdoors in Demonstrations:} We evaluate in-context learning results of backdoored demonstrations,  with the goal of evaluating if such manipulated demonstrations can mislead the model.         
    \end{itemize}
\subsubsection*{Privacy}
        \begin{itemize}
        \item \textbf{Enron Email Extraction:} We evaluate the information extraction accuracy of sensitive information in pretraining data such as the Enron email dataset \cite{klimt2004enron} to evaluate the model’s memorization problem of training data.
        \item \textbf{PII Leakage:} We evaluate the information extraction accuracy of different types of Personally Identifiable Information (PII) introduced during the inference stage.
        \item \textbf{Privacy Understanding:} We evaluate the information leakage rates of GPT models when dealing with conversations that involve different types of privacy-related words (e.g., confidentially) and privacy events (e.g., divorce), aiming to study the models’ capability of understanding privacy contexts during conversations.
    \end{itemize}
\subsubsection*{Machine Ethics}
\begin{itemize}
        \item \textbf{Standard Benchmarks:} We evaluate on standard benchmarks ETHICS \cite{ethics} and Jiminy Cricket \cite{jiminy}, aiming to assess the model performance of moral recognition.
        \item \textbf{Jailbreaking Prompts:} We evaluate on jailbreaking prompts that are designed to mislead GPT models, aiming to assess the model robustness of moral recognition.
        \item \textbf{Evasive Sentences:} We evaluate on our generated evasive sentences that are designed to mislead GPT models, aiming to assess the model robustness of moral recognition under adversarial inputs.
    \end{itemize}
\subsubsection*{Fairness}
\begin{itemize}
    \item  \textbf{Zero-shot setting:} We evaluate test groups with different base rate parity in zero-shot settings, aiming to explore whether models have large performance gaps across these test groups.
    \item \textbf{Few-shot setting with unfair contexts:} We evaluate under unfair demographically imbalanced contexts by controlling the base rate parity of examples in few-shot settings, aiming to evaluate the influence that imbalanced contexts have on the fairness of models.
    \item \textbf{Few-shot setting with fair contexts:} We evaluate under different numbers of fair demographically balanced examples, aiming to study how the fairness of models is affected by providing a more balanced context.
\end{itemize}
\subsubsection*{Harmfulness}
\begin{itemize}
    \item  TBD
\end{itemize}
\newpage

